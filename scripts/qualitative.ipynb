{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablating Token Induction Heads\n",
    "In Section 4.2, we show that ablating token induction heads leads to paraphrasing behavior in model generation. This notebook is the one we used to generate the examples in the paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random \n",
    "import numpy as np \n",
    "import torch \n",
    "import json \n",
    "from nnsight import LanguageModel\n",
    "from utils import get_mean_head_values\n",
    "\n",
    "random.seed(10)\n",
    "torch.manual_seed(10)\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.26s/it]\n"
     ]
    }
   ],
   "source": [
    "full_model = 'meta-llama/Llama-2-7b-hf'\n",
    "# full_model = 'meta-llama/Meta-Llama-3-8B'\n",
    "# full_model = 'allenai/OLMo-2-1124-7B'\n",
    "# full_model = 'EleutherAI/pythia-6.9b'\n",
    "model_name = full_model.split('/')[-1]\n",
    "model = LanguageModel(full_model, device_map='cuda', dispatch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../cache/head_orderings/{model_name}/token_copying.json', 'r') as f:\n",
    "    token_heads = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_means = get_mean_head_values(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ablated_generation(model, sequences, heads_to_ablate, head_means, max_toks=10):\n",
    "    n_heads = model.config.num_attention_heads \n",
    "    head_dim = model.config.hidden_size // n_heads \n",
    "\n",
    "    layers_in_order = sorted(list(set([layer for layer, _ in heads_to_ablate])))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with model.generate(sequences, max_new_tokens=max_toks):\n",
    "            model.all()\n",
    "            for curr_layer in layers_in_order:\n",
    "                if model_name == 'pythia-6.9b':\n",
    "                    o_proj = model.gpt_neox.layers[curr_layer].attention.dense\n",
    "                else:\n",
    "                    o_proj = model.model.layers[curr_layer].self_attn.o_proj\n",
    "                    \n",
    "                # [bsz, seq_len, model_dim]\n",
    "                o_proj_inp = o_proj.inputs[0][0]\n",
    "                \n",
    "                # get activations for the last token [model_dim], and then \n",
    "                # reshape into heads [bsz, seq_len, model_dim] -> [bsz, seq_len, n_heads, head_dim=128]\n",
    "                bsz = o_proj_inp.shape[0]; seq_len = o_proj_inp.shape[1]\n",
    "                head_acts = o_proj_inp.view(bsz, seq_len, n_heads, head_dim)\n",
    "                \n",
    "                curr_heads = [head for layer, head in heads_to_ablate if layer == curr_layer]\n",
    "                for h in curr_heads:\n",
    "                    the_mean = head_means[curr_layer, h]\n",
    "                    head_acts[:, :, h, :] = the_mean.cuda()\n",
    "            \n",
    "                # replace the output of self_attn.q_proj with modified vector\n",
    "                new_guy = ((head_acts.reshape(bsz, seq_len, model.config.hidden_size),),{})\n",
    "                o_proj.inputs = new_guy\n",
    "            \n",
    "            out = model.generator.output.save()\n",
    "    \n",
    "        return out.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos = model.tokenizer.bos_token_id\n",
    "if 'meta-llama' in model_name:\n",
    "    newline = model.tokenizer('\\n')['input_ids'][1:][-1]\n",
    "else:\n",
    "    newline = model.tokenizer('\\n')['input_ids'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ablated_token_generation(prompt, k=32, repeat_cutoff=1, repeated_ratio=1.3):\n",
    "    if 'Llama' in model_name:\n",
    "        custom_chunk = model.tokenizer(prompt)['input_ids'][1:]\n",
    "    else:\n",
    "        custom_chunk = model.tokenizer(prompt)['input_ids']\n",
    "    copy_prompt = [bos] + custom_chunk + [newline] + custom_chunk + [newline] + custom_chunk[:repeat_cutoff]\n",
    "    copy_paragraph = [copy_prompt]\n",
    "    generated = ablated_generation(model, copy_paragraph, token_heads[:k], head_means, max_toks=math.floor(len(custom_chunk)*repeated_ratio))\n",
    "    print('original:')\n",
    "    print(model.tokenizer.decode(copy_prompt))\n",
    "    print('\\n')\n",
    "\n",
    "    print('generated:')\n",
    "    print(model.tokenizer.batch_decode(generated)[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original:\n",
      "<s> foo = []\n",
      "for i in range(len(bar)):\n",
      "    if i % 2 == 0:\n",
      "        foo.append(bar[i])\n",
      "\n",
      " foo = []\n",
      "for i in range(len(bar)):\n",
      "    if i % 2 == 0:\n",
      "        foo.append(bar[i])\n",
      "\n",
      " foo =\n",
      "\n",
      "\n",
      "generated:\n",
      "<s> foo = []\n",
      "for i in range(len(bar)):\n",
      "    if i % 2 == 0:\n",
      "        foo.append(bar[i])\n",
      "\n",
      " foo = []\n",
      "for i in range(len(bar)):\n",
      "    if i % 2 == 0:\n",
      "        foo.append(bar[i])\n",
      "\n",
      " foo = [item for sublist in bar for item in sublist if i % 2 == 0]\n",
      "print(foo)\n",
      "\\end{code}\n",
      "\n",
      "Comment: Thank you for your answer, I have edited\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"foo = []\n",
    "for i in range(len(bar)):\n",
    "    if i % 2 == 0:\n",
    "        foo.append(bar[i])\n",
    "\"\"\"\n",
    "# ablated_token_generation(prompt, k=0, repeat_cutoff=2) # make sure it actually copies \n",
    "ablated_token_generation(prompt, repeat_cutoff=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original:\n",
      "<s> I have reread, not without pleasure, my comments to his lines, and in many cases have caught myself borrowing a kind of opalescent light from my poet's fiery orb\n",
      " I have reread, not without pleasure, my comments to his lines, and in many cases have caught myself borrowing a kind of opalescent light from my poet's fiery orb\n",
      " I\n",
      "\n",
      "\n",
      "generated:\n",
      "<s> I have reread, not without pleasure, my comments to his lines, and in many cases have caught myself borrowing a kind of opalescent light from my poet's fiery orb\n",
      " I have reread, not without pleasure, my comments to his lines, and in many cases have caught myself borrowing a kind of opalescent light from my poet's fiery orb\n",
      " I have reread my comments on his lines, and I have caught myself many times borrowing from his fiery orb a kind of opalescent light.\n",
      "—T.S. Eliot, letter to Ezra Pound, 26 November\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I have reread, not without pleasure, my comments to his lines, and in many cases have caught myself borrowing a kind of opalescent light from my poet's fiery orb\"\n",
    "ablated_token_generation(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original:\n",
      "<s> English vocab:\n",
      " 1. hospital\n",
      " 2. visage\n",
      " 3. unlucky\n",
      " 4. lesbian\n",
      " 5. know\n",
      " 6. wizard\n",
      " 7. unfair\n",
      " 8. sound\n",
      " 9. cortical\n",
      " 10. obsolete\n",
      " English vocab:\n",
      " 1. hospital\n",
      " 2. visage\n",
      " 3. unlucky\n",
      " 4. lesbian\n",
      " 5. know\n",
      " 6. wizard\n",
      " 7. unfair\n",
      " 8. sound\n",
      " 9. cortical\n",
      " 10. obsolete\n",
      " English\n",
      "\n",
      "\n",
      "generated:\n",
      "<s> English vocab:\n",
      " 1. hospital\n",
      " 2. visage\n",
      " 3. unlucky\n",
      " 4. lesbian\n",
      " 5. know\n",
      " 6. wizard\n",
      " 7. unfair\n",
      " 8. sound\n",
      " 9. cortical\n",
      " 10. obsolete\n",
      " English vocab:\n",
      " 1. hospital\n",
      " 2. visage\n",
      " 3. unlucky\n",
      " 4. lesbian\n",
      " 5. know\n",
      " 6. wizard\n",
      " 7. unfair\n",
      " 8. sound\n",
      " 9. cortical\n",
      " 10. obsolete\n",
      " English vocabulary:\n",
      "1. She was in hospital for a long time after the accident.\n",
      "2. Her face was covered in bandages.\n",
      "3. The accident was very unlucky.\n",
      "4. She was a lesbian.\n",
      "5. He knows everything about the wizard.\n",
      "6. It is unfair to treat people like that.\n",
      "7. The sound of the engine was very loud.\n",
      "8. I don't know why the doctor told me that I should eat more fish.\n",
      "9. The new cortical implants can help the blind to see.\n",
      "10. The\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"English vocab:\\n 1. hospital\\n 2. visage\\n 3. unlucky\\n 4. lesbian\\n 5. know\\n 6. wizard\\n 7. unfair\\n 8. sound\\n 9. cortical\\n 10. obsolete\"\n",
    "ablated_token_generation(prompt, repeated_ratio=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
